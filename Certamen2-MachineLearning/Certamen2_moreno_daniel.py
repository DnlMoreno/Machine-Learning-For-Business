# -*- coding: utf-8 -*-
"""Certamen2_MorenoDaniel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mU7bIfboPy9evgef03vSqTQ9BlYsu_wP

# Certamen 2: Machine Learning for Business Intelligence.

**Integrante:** Daniel Moreno Cartagena

**Profesor:** Rodrigo De La Fuente

## Instrucciones

1. Este certamen es de caracter individual, por lo tanto queda estrictamente prohibida la copia de codigo ajeno. **Cualquier copia será motivo de reprobación (esto excluye código mostrado en laboratorios prácticos).**

2. En caso de elaborar su respuesta **basandose** en código ajeno encontrado en la web, debe agradecer al menos citando la referencia.

3. La fecha límite para la entrega de este certamen queda establecida para el **22/08/2020 a la media noche**.

Nota: La resolución de algunos de los ejercicios puede tomar bastante tiempo, así como su ejecución. Por lo tanto se recomienda hacer la tarea con anticipación y evitar riesgos. Además, es **obligatorio el uso de Google Colab**.

## Pregunta 1: Convolutional Generative Adversarial Nets (CGAN).

En esta sección, se pide reproducir el modelo [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets) (Goodfellow I., et al. 2014), utilizando redes convolucionales. Para esto se utilizará el dataset Fashion MNIST, que consiste en prendas de ropa en blanco y negro.

1.1 Cargue los datos utilizando la ayuda de su [https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data](documentación). Muestre al menos 1 imagen de cada clase, pertenecientes al conjunto de entrenamiento (2 ptos).
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import keras

from keras.initializers import RandomNormal
from keras.models import Sequential
from keras.models import Model
from keras import backend as K
from keras.datasets import mnist
from keras import losses, optimizers
from keras.callbacks import EarlyStopping
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import Input, Dense, Lambda, Conv2D, Flatten, Conv2DTranspose, Reshape, BatchNormalization, Concatenate, Dropout

from keras.datasets.fashion_mnist import load_data

#Se cargan los datos
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#Se muestra al menos una imagen de las clases del conjunto de entrenamiento
fig = plt.figure(figsize=(24,5))
i=0
for i, (image, label) in enumerate(zip(x_train[0:24], y_train[0:24])):
  ax = plt.subplot(2, 12, i + 1)
  plt.imshow(np.reshape(image, (28,28)), cmap='gray')
  ax.xaxis.set_visible(False)
  ax.yaxis.set_visible(False)
  plt.title('Training: %i\n' % label, fontsize = 13)

"""1.2 En clases se vieron ciertas técnicas para mejorar el entrenamiento de los modelos generativos adversarios [(ref)](https://arxiv.org/abs/1606.03498). Dentro de ellas estaba la normalización de los datos entre [-1,1]. Normalice el conjunto de entrenamiento entre ese rango. Además, añada una dimensión extra que permita el uso de redes convolucionales 2D (como se vio en clases) (3 ptos)."""

#Normalización entre [-1,1]
x_train =  x_train.astype('float32')/255
x_train = x_train*2 - 1
print(x_train.max(),x_train.min())

x_train.shape

#Se añade dimensión extra
x_train = np.expand_dims(x_train, axis=3)
x_train.shape

"""1.3 Construya una arquitectura convolucional que permita el entrenamiento del modelo (generador y discriminador), siguiendo los consejos vistos en clases (ej. batch normalization, leaky relu). Para este paso, es posible utilizar como base la arquitectura encoder-decoder vista en clase práctica (sin embargo, debe tener en cuenta que esa arquitectura recibe como input una imagen de (1, 32, 32), por lo cual sería necesario redimensionar las imágenes del conjunto de entrenamiento previamente) (5 ptos)."""

latent_dim = 100 #Hiperparámetro de la pregunta 1.4

#Generador
generator = Sequential()
generator.add(Dense(7*7*256, kernel_initializer=RandomNormal(0, 0.02), input_dim=latent_dim))
generator.add(LeakyReLU(alpha=0.2))    
generator.add(Reshape((7, 7, 256)))
generator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))
generator.add(LeakyReLU(alpha=0.2))
generator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))
generator.add(LeakyReLU(alpha=0.2))
generator.add(Conv2D(1, (3, 3), padding='same', activation='tanh', kernel_initializer=RandomNormal(0, 0.02)))

generator.summary()

#Discriminador
discriminator = Sequential()
discriminator.add(Conv2D(64, kernel_size=5, strides=2, padding="same", activation=LeakyReLU(0.3), input_shape=[28, 28, 1]))
discriminator.add(Dropout(0.5))
discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding="same", activation=LeakyReLU(0.3)))
discriminator.add(Dropout(0.5))
discriminator.add(Flatten())
discriminator.add(Dense(1, activation="sigmoid", input_shape=[28, 28, 1]))

discriminator.summary()

"""1.4 Entrene el modelo generativo adversario utilizando optimizador Adam y los siguientes hiperparámetros (5 ptos):
- learning_rate=0.0002
- beta_1=0.5
- latent_dim=100
- num_epochs=1000.

1.5 Muestre imágenes generadas en la epoca 10, 100, 500 y 1000 (5 ptos).

"""

### Las imagenes de las epocas 10, 100, 500 y 1000 de la pregunta 1.5 se iran mostrando durante el entrenamiento. ###

#Hiperparámetros
learning_rate = 0.0002
beta_1 = 0.5
num_epochs = 1000
batch_size = 32
noise_shape = 100
n_classes = 10
steps_per_epoch = 10

GAN = Sequential([generator,discriminator])
adam = keras.optimizers.Adam(learning_rate, beta_1)

discriminator.compile(optimizer=adam ,loss='binary_crossentropy', metrics=['accuracy'])
discriminator.trainable = False

GAN.compile(optimizer=adam ,loss='binary_crossentropy')
GAN.layers
GAN.summary()

y_train = keras.utils.to_categorical(y_train, num_classes=n_classes)

#https://www.kaggle.com/sayakdasgupta/introduction-to-gans-on-fashion-mnist-dataset

#Entrenamiento del modelo
gen_losses = []
disc_losses = []

for epoch in range(num_epochs):
    print(f"Currently on Epoch {epoch+1}")
    for i in range(steps_per_epoch):           
        noise = np.random.normal(0, 1, (batch_size, latent_dim))       
        fake_imgs = generator.predict_on_batch(noise)
        imgs = x_train[i*batch_size:(i+1)*batch_size]
       
        #training discriminator on real images
        true_labels=np.ones((batch_size,1))
        discriminator.trainable = True
        d_loss_real=discriminator.train_on_batch(imgs, true_labels)
        
        #training discriminator on fake images
        fake_labels=np.zeros((batch_size,1))
        d_loss_fake=discriminator.train_on_batch(fake_imgs, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        #training generator 
        noise=np.random.normal(0, 1, (batch_size, latent_dim))
        discriminator.trainable = False 
        g_loss =GAN.train_on_batch(noise, true_labels)  
    
    print('Discriminator loss: {:.3f}, Generator loss: {:.3f}'.format(d_loss[0], g_loss))
    gen_losses.append(g_loss)
    disc_losses.append(d_loss[0])

    #Muestra las imagenes generadas en las epocas 10, 100, 500 y 1000
    if epoch==9 or epoch==99 or epoch==499 or epoch==999:
        samples = 10
        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, 100)))
        for k in range(samples):
            plt.subplot(2, 5, k+1)
            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')
            plt.xticks([])
            plt.yticks([])
        plt.tight_layout()
        plt.show()
print('Training is complete')

#Generador de imagenes desde noise
r, c = 5, 5
noise = np.random.normal(0, 1, (r * c, latent_dim))
gen_imgs = generator.predict(noise)

# Rescale images 0 - 1
gen_imgs = 0.5 * gen_imgs + 0.5

fig, axs = plt.subplots(r, c, figsize=(10,10))
fig.suptitle('Generated Images from Noise', fontsize = 35)

ix = 0
for i in range(r):
    for j in range(c):
        axs[i,j].imshow(gen_imgs[ix].reshape(28,28), cmap='gray')
        axs[i,j].axis('off')
        ix += 1
plt.show()

"""## Pregunta 2: Recurrent Neural Networks (RNN)

Imagine que llega a su nuevo trabajo, el cuál obtuvo luego de una muy buena entrevista en la que deleitó a su actual jefe con sus conocimientos de Deep Learning. A su sorpresa, llega uno de sus colegas y le comienza a decir que estan trabajando en un sistema de pronósticos del precio de acciones transadas en la Bolsa de Santiago, pero que hasta el momento no han podido utilizar algo mas complejo que un MLP. Le explican que ellos sólo desfasan los valores de apertura de las acciones por unos cuantos días (usted vió su código y son 5 días exáctamente), tratando de imitar el concepto de autocorrelación propio de las series de tiempo. 

Como usted quiere demostrar su valía como nueva contratación les pide que le pasen información histórica de cuatro acciones: Enel Americas, Copec, CMPC, y Engie Energía. También les dice que usted generará un modelo integrado que recibe información histórica de las 4 series y predice al mismo tiempo los 4 valores para el día siguiente. Además, como su emoción es tan grande, se compromete en comparar sus resultados con el modelo que ellos tienen implementado. 

Adicionalmente, ellos le comentan que les interesaría que los modelo sean entrenado en los que terminan el 12/29/2017, y que el set de testeo comience el día inmediatamente siguiente. Además, usted recuerda que k-fold Cross Validation no funciona para las series de tiempo, por lo que debe utilizar Walk-Forward validation para entrenar su modelo y elegir los mejores hiper-parámetros. Finalmente, le enviaron un correo que contiene el archivo aquiVanLosDatosCompadre.zip

###Preparación de la data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import numpy

from numpy import nan
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

from keras.optimizers import Adam

from numpy import array
from numpy import hstack
from keras.layers import Dense

from tensorflow import keras
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive/')

data_ecl = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Certamen 2/ECL.csv', sep=',', parse_dates=True, low_memory=False, header=None)
data_cmpc = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Certamen 2/CMPC.csv', sep=',', parse_dates=True, low_memory=False, header=None)
data_copec = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Certamen 2/COPEC.csv', sep=',', parse_dates=True, low_memory=False)
data_enelam = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Certamen 2/ENELAM.csv', sep=',', parse_dates=True, low_memory=False, header=None)

print(data_ecl.head())
print(data_cmpc.head())
print(data_copec.head())
print(data_enelam.head())

data_ecl.columns = ['Date', 'Open']
data_cmpc.columns = ['Date', 'Open']
data_enelam.columns = ['Date', 'Open']

#Division de la data de entrenamiento y testeo
ecl_train = data_ecl[data_ecl['Date']<='2017-12-29'].copy()
cmpc_train = data_cmpc[data_ecl['Date']<='2017-12-29'].copy()
copec_train = data_copec[data_ecl['Date']<='2017-12-29'].copy()
enelam_train = data_enelam[data_ecl['Date']<='2017-12-29'].copy()

ecl_test = data_ecl[data_ecl['Date']>'2017-12-29'].copy()
cmpc_test = data_cmpc[data_ecl['Date']>'2017-12-29'].copy()
copec_test = data_copec[data_ecl['Date']>'2017-12-29'].copy()
enelam_test = data_enelam[data_ecl['Date']>'2017-12-29'].copy()

print(ecl_train.shape)
print(cmpc_train.shape)
print(copec_train.shape)
print(enelam_train.shape)

print(ecl_test.shape)
print(cmpc_test.shape)
print(copec_test.shape)
print(enelam_test.shape)

copec_train

#Se eliman las columnas que ya no se van a ocupar
train_ecl = ecl_train.drop(['Date'], axis=1)
train_cmpc = cmpc_train.drop(['Date'], axis=1)
train_copec = copec_train.drop(['Date', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], axis=1)
train_enelam = enelam_train.drop(['Date'], axis=1)

test_ecl = ecl_test.drop(['Date'], axis=1)
test_cmpc = cmpc_test.drop(['Date'], axis=1)
test_copec = copec_test.drop(['Date', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], axis=1)
test_enelam = enelam_test.drop(['Date'], axis=1)

#https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecas

# Se divide la secuencia multivariante en muestras
def split_sequences(sequences, n_steps):
	X, y = list(), list()
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the dataset
		if end_ix > len(sequences)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# Se define la secuencia de entrada de entrenamiento y test en vector
in_seq1 = array(train_ecl)
in_seq2 = array(train_cmpc)
in_seq3 = array(train_copec)
in_seq4 = array(train_enelam)

in_test1 = array(test_ecl)
in_test2 = array(test_cmpc)
in_test3 = array(test_copec)
in_test4 = array(test_enelam)

# Se conviente la estuctura en [rows, columns]
in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))

in_test1 = in_test1.reshape((len(in_test1), 1))
in_test2 = in_test2.reshape((len(in_test2), 1))
in_test3 = in_test3.reshape((len(in_test3), 1))
in_test4 = in_test4.reshape((len(in_test4), 1))

# Se apilan las columnas horizontalmente
dataset_train = hstack((in_seq1, in_seq2, in_seq3, in_seq4))
dataset_test = hstack((in_test1, in_test2, in_test3, in_test4))

# Se convierten en datos de entrada y salida
n_steps=5
X_train, Y_train = split_sequences(dataset_train, n_steps)
X_test, Y_test = split_sequences(dataset_test, n_steps)

X_train.shape

#Se ocupara en la entrada del LSTM
xlstm_train = X_train
xlstm_test = X_test

"""1. Implementación, selección de hyperparámetros, y entrenamiento del MLP (5 ptos)."""

# Se aplanan los datos para ingresarlos a la red
n_input_train = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape((X_train.shape[0], n_input_train))
n_output_train = Y_train.shape[1]

n_input_test = X_test.shape[1] * X_test.shape[2]
X_test = X_test.reshape((X_test.shape[0], n_input_test))
n_output_test = Y_test.shape[1]

#Hiperparámetros
input_dim = n_input_train
lr = 0.001
epochs = 200
batch_size = 16
n_output = n_output_train

X_train.shape

# Definición del modelo MLP
mlp_model = Sequential()
mlp_model.add(Dense(128, activation='relu', input_dim=input_dim))
mlp_model.add(Dense(64, activation='relu'))
mlp_model.add(Dense(32, activation='relu'))
mlp_model.add(Dense(n_output))
mlp_model.summary()
opt = keras.optimizers.Adam(learning_rate=lr)
mlp_model.compile(optimizer=opt, loss='mean_squared_error')

#Entrenamiento del modelo
history = mlp_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=2)

"""2. Implementación, selección de hyperparámetros, y entrenamiento del LSTM RNN (10 ptos)."""

#Hiperparámetros
learning_rt = 0.001
in_units = 128
num_epoch = 200
batch_sz = 16

xdata_train, ydata_train = np.array(xlstm_train), np.array(Y_train)
print(xdata_train.shape, ydata_train.shape)

xdata_test, ydata_test = np.array(xlstm_test), np.array(Y_test)
print(xdata_test.shape, ydata_test.shape)

### Building LSTM ####

lstm_model = Sequential()

lstm_model.add(LSTM(units=in_units, activation='relu', return_sequences=True, input_shape=(xdata_train.shape[1], xdata_train.shape[1]-1)))
#model.add(Dropout(0.1))

lstm_model.add(LSTM(units=64, activation='relu', return_sequences=True))
#model.add(Dropout(0.1))

lstm_model.add(LSTM(units=32, activation='relu', return_sequences=True))
#model.add(Dropout(0.1))

lstm_model.add(LSTM(units=16, activation='relu'))
#model.add(Dropout(0.1))

lstm_model.add(Dense(units=4))

lstm_model.summary()

opt1 = keras.optimizers.Adam(learning_rate=learning_rt)
lstm_model.compile(optimizer=opt1, loss = 'mean_squared_error')

lstm_model.fit(xdata_train, ydata_train, batch_size = batch_sz, epochs=num_epoch, verbose=2)

"""3. Evaluación de métricas de desempeño durante el periodo de testeo y comparación del rendimiento de ambos modelos (5 ptos)."""

#Visualización de la prediccion de los valores del set de entrenamiento MLP
y_pred = mlp_model.predict(X_train, verbose=0)
print('MLP Model predict:')
print(y_pred)
print('')
#Datos Reales
print('Real Values:')
print(Y_train)
print('')
#Visualización de la prediccion de los valores del set de entrenamiento LSTM
y1_pred = lstm_model.predict(xdata_train, verbose=0)
print('LSTM Model predict:')
print(y1_pred)

# Estimación del rendimiento del modelo MLP
trainScore_m = mlp_model.evaluate(X_train, Y_train, verbose=0)
print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore_m, math.sqrt(trainScore_m)))
testScore_m = mlp_model.evaluate(X_test, Y_test, verbose=0)
print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore_m, math.sqrt(testScore_m)))

# Estimación del rendimiento del modelo LSTM
trainScore_l = lstm_model.evaluate(xdata_train, Y_train, verbose=0)
print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore_l, math.sqrt(trainScore_l)))
testScore_l = lstm_model.evaluate(xdata_test, Y_test, verbose=0)
print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore_l, math.sqrt(testScore_l)))

# Generate predictions for training MLP
trainPredict_m = mlp_model.predict(X_train)
testPredict_m = mlp_model.predict(X_test)

# Generate predictions for training LSTM
trainPredict_l = lstm_model.predict(xdata_train)
testPredict_l = lstm_model.predict(xdata_test)

data_ecl = data_ecl.drop(['Date'], axis=1)
data_cmpc = data_cmpc.drop(['Date'], axis=1)
data_copec = data_copec.drop(['Date', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], axis=1)
data_enelam = data_enelam.drop(['Date'], axis=1)
dataset = hstack((data_ecl, data_cmpc, data_copec, data_enelam))

dataset.shape

#https://machinelearningmastery.com/time-series-prediction-with-deep-learning-in-python-with-keras/

# Shift train predictions for plotting of MLP
trainPredict_mPlot = numpy.empty_like(dataset)
trainPredict_mPlot[:, :] = numpy.nan
trainPredict_mPlot[n_steps:len(trainPredict_m)+n_steps, :] = trainPredict_m

# Shift test predictions for plotting of MLP
testPredict_mPlot = numpy.empty_like(dataset)
testPredict_mPlot[:, :] = numpy.nan
testPredict_mPlot[len(trainPredict_m)+(n_steps*2):len(dataset), :] = testPredict_m

# Shift train predictions for plotting of LSTM
trainPredict_lPlot = numpy.empty_like(dataset)
trainPredict_lPlot[:, :] = numpy.nan
trainPredict_lPlot[n_steps:len(trainPredict_l)+n_steps, :] = trainPredict_l

# Shift test predictions for plotting of LSTM
testPredict_lPlot = numpy.empty_like(dataset)
testPredict_lPlot[:, :] = numpy.nan
testPredict_lPlot[len(trainPredict_l)+(n_steps*2):len(dataset), :] = testPredict_l

# Plot baseline and predictions
plt.figure(figsize=(25,14))

ax1 = plt.subplot(2, 2, 1)
plt.plot(dataset)
plt.plot(trainPredict_mPlot)
plt.title('Predicciones del entrenamiento de la MLP', fontsize=25)

ax1 = plt.subplot(2, 2, 2)
plt.plot(dataset)
plt.plot(testPredict_mPlot)
plt.title('Predicciones del testeo de la MLP', fontsize=25)

ax1 = plt.subplot(2, 2, 3)
plt.plot(dataset)
plt.plot(trainPredict_lPlot)
plt.title('Predicciones del entrenamiento de la LSTM', fontsize=25)

ax1 = plt.subplot(2, 2, 4)
plt.plot(dataset)
plt.plot(testPredict_lPlot)
plt.title('Predicciones del testeo de la LSTM', fontsize=25)

plt.show()